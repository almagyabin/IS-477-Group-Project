# **FINAL PROJECT REPORT: Anime Exploration**

## **Contributors**
Obi Obah | Alma Gyabin

## **Summary**
Our project focuses on the complete curation, integration, cleaning, and analysis of anime metadata in order to answer two research questions related to user ratings and favorites. We decided to examine anime rating behavior because anime datasets contain rich metadata, high user activity, and well structured rating systems. This made anime an ideal domain for exploring data curation techniques while also giving us the opportunity to investigate user engagement patterns. Our goal was to create a full workflow that begins with raw data acquisition and ends with reproducible results that can be understood, verified, and repeated by someone else. We wanted to demonstrate that thoughtful planning, documentation, and structure can transform unprocessed datasets into meaningful insights.

We selected two datasets that both originate from the JIKAN API, which is derived from MyAnimeList. The first dataset is a csv file that includes numerical fields such as rating scores, favorites, popularity indicators, and episode counts. The second dataset is a JSON file that includes expanded descriptive information about each anime title. We chose these two sources because they complement each other and because they represent two different data formats that require different handling techniques. By working with both formats, we were able to practice integration, schema alignment, and data understanding across multiple structures. This choice also allowed us to build a pipeline that handles realistic data challenges.

Our two research questions guided every stage of our work. The first question asks: What is the most common user rating score assigned to anime? This question encourages us to examine rating distributions at a large scale and to understand whether users tend to rate shows positively, neutrally, or negatively. It also creates a foundation for interpreting how the community interacts with anime titles. The second research question asks: Does a higher rating correlate with having more favorites? This question requires us to look beyond single variable summaries and instead explore relationships between variables. Favorites reflect a more personal connection than a simple rating score, so studying this relationship helps us understand whether the rating system aligns with deeper user engagement.

We structured our entire workflow using Snakemake because it allows us to define each step in an ordered and reproducible manner. This structure ensures that nothing is processed out of sequence and that every step depends on the correct inputs. As a result, our workflow became a clear and transparent pipeline that moves from raw data to results in a consistent way. Snakemake helped us understand how reproducibility requires deliberate design and careful documentation. It also reminded us that reproducibility becomes easier when the workflow is automated.

We also learned that raw data must be treated with caution. Even datasets that appear clean at first glance often contain inconsistencies, missing values, and structural issues. We spent time evaluating the quality of both datasets so that we could make informed decisions about cleaning strategies. We identified missing ratings and favorites, fields that required type conversions, and outliers that needed to be reviewed carefully. This stage helped us appreciate how essential data quality assessment is for any meaningful analysis.

Throughout the project, collaboration played an important role in our progress. We divided tasks based on our timeline and met regularly to update each other. One of us focused on integration and merging, while the other concentrated on cleaning and workflow automation. By working together and staying organized, we were able to produce a complete project with strong documentation, structured folders, and consistent naming conventions. This collaboration also helped us understand that successful reproducible research depends not only on technical tools but also on clear communication.

Our project ultimately allowed us to answer our research questions and produce visualizations that reflect the patterns we discovered. Beyond the results themselves, we gained valuable experience working with reproducible workflows, dataset licenses, metadata, and documentation. This project showed us the importance of transparent research practices and how structured approaches can turn raw data into clear analytical outcomes.

## **Data Profile**
Our project uses two datasets that provide complementary information about anime titles. These datasets represent different formats and structures, which gave us an opportunity to learn how to integrate diverse data sources. Together, they allowed us to answer our research questions while also practicing skills related to acquisition, documentation, ethics, and quality assessment.

The first dataset, anime.csv, is a structured csv file originally obtained from Kaggle. The csv format is simple and easy to work with, which made it an ideal starting point for our analysis. The dataset includes thousands of anime titles along with fields such as rating score, number of favorites, popularity ranking, number of episodes, and other numerical or categorical data. The rating score and favorites fields were the two variables required for our research questions. Because the dataset comes from Kaggle, we reviewed the license information on the dataset page and identified that the data is allowed for analysis but may not be redistributed. To respect these terms, we did not store the raw csv file in our GitHub repository. Instead, we provide a Box link for teaching staff to download the file, and we require users to place the file into the data/raw directory.

The second dataset, anime_full_data.json, comes from HuggingFace and contains descriptive metadata in JSON format. JSON files can represent complex nested structures, lists, and objects, which makes them useful for storing richer information than a csv file. This dataset includes fields such as English titles, Japanese titles, alternative names, genres, theme tags, demographic categories, production studios, and related shows. Although our research questions did not require every field, the JSON dataset helped us validate entries, understand naming consistency, and practice merging two datasets. We also examined the license on HuggingFace and found that the data is available for analysis. To follow proper usage rules, we do not redistribute the raw JSON file either. Instead, we automate the download through our acquire_data.py script.

Both datasets trace back to the same API ecosystem, which helped ensure consistency between identifiers. Many entries share matching or similar titles, and both datasets reflect metadata that originates from MyAnimeList. This allowed us to create a unified dataset after merging. We document the relationship between these two sources in our report so that future users understand why these datasets can be integrated.

Ethical and legal considerations were important throughout the data profile stage. Even though the datasets do not include personal user data, they still carry usage restrictions. We followed all expectations by providing SHA 256 hashes for both files, storing data outside GitHub, and documenting their original locations. We also made sure to cite each dataset properly in our references section. While ethical risks are low due to the nature of the data, we still needed to ensure transparency about licenses and our handling procedures.

Metadata and documentation play a central role in making our datasets understandable to others. We created clear descriptions of each dataset, including file formats, variable types, and instructions for where the files should be placed. We also provide guidance for reproduction so that users can recreate the exact data environment that we used in our workflow. This level of documentation supports the FAIR principles by helping future users understand the origin, structure, and purpose of the data.

By selecting two datasets with different structures and origins, we challenged ourselves to create a workflow that accommodates diverse formats. This choice strengthened our understanding of data integration and helped us build a pipeline that reflects real world data challenges. Our data profile demonstrates that even when datasets appear straightforward, they must be handled carefully in order to respect licenses, maintain structure, and preserve reproducibility.

## **Data Quality**
Our data quality assessment focused on evaluating completeness, consistency, structure, and accuracy across both datasets. We treated data quality as an essential step in the workflow because any errors or inconsistencies at this stage directly influence the reliability of the final analysis. By examining each dataset independently and then together, we improved our understanding of how they could be used effectively.

For anime.csv, we began by reviewing the structure of the file. We checked for missing values in key fields such as rating score and favorites. We found that while most entries contained valid values, some rows were incomplete. Missing ratings and missing favorites could distort our analysis because they might be interpreted as zeros or incorrect values if not addressed. We chose to remove rows with missing key fields and documented this choice clearly in our cleaning script.

Next, we reviewed data types. Some numeric fields appeared as strings due to formatting inconsistencies. We corrected these by casting them to numeric types. Ensuring correct types allowed us to compute statistics, analyze distributions, and create visualizations without errors.

We also examined the dataset for duplicates. Because anime titles sometimes appear with slight variations, we checked for repeated identifiers. We did not find significant duplication, but we documented the process to maintain transparency.

Outliers in favorites and ratings required careful attention. Some entries had extremely high favorite counts. Instead of removing these values, we reviewed the titles and confirmed that the results reflected legitimate popularity for certain well known shows. This helped us avoid accidentally removing meaningful data.

For the JSON dataset, we reviewed the structure of each record. JSON files can contain nested lists, objects, and variable length fields. We inspected several entries to ensure that the fields we needed for integration were consistent. Although some entries had missing descriptions or incomplete lists, these gaps did not affect the fields used in our integration process. We documented all structural issues and explained why they did not impact our research questions.

Consistency between datasets was another important part of data quality. We needed to confirm that entries from one dataset matched entries in the other. Titles sometimes appear under multiple alternative names, which made matching more complex. Fortunately, shared identifiers and alignment methods allowed us to merge the datasets accurately.

After identifying all issues, we developed a cleaning strategy that involved filtering incomplete rows, converting data types, selecting relevant fields, and preparing the structure for analysis. Cleaning actions were documented in detail to support reproducibility. We learned that proper data quality assessment not only improves the final results but also strengthens confidence in the entire workflow.

## **Findings**
Our findings focus on understanding how anime users behave when rating shows and marking them as favorites. By examining the most common user rating score and the relationship between ratings and favorites, we were able to interpret meaningful patterns in the dataset. We approached our findings from both descriptive and analytical perspectives, which allowed us to view user engagement from more than one angle.

Our first finding relates to the distribution of rating scores. When we examined the frequency of each score, we discovered that most anime titles cluster in the mid to high score range. Very few titles fall into the low score range. This indicates that users generally rate anime positively. One possible interpretation is that many users avoid giving low ratings unless they strongly dislike a show. Another possibility is that users tend to watch shows they already expect to enjoy, so their ratings naturally skew higher. Our histogram visualization highlights this pattern clearly. The distribution has a noticeable peak in the higher ranges, which reinforces the idea that favorable ratings are the norm.

We also observed that the distribution is not uniform and that certain score values appear far more frequently than others. This could reflect user tendencies, rating habits, or properties of the rating system itself. The most common rating gives us a baseline that helps us compare shows, because any show that significantly deviates from this common score might represent something unusual.

Our second major finding involves the relationship between rating scores and the number of favorites. We created a scatterplot to visualize this relationship, and we calculated a correlation coefficient to quantify it. The scatterplot reveals a clear positive trend. Higher rated anime titles tend to have more favorites, while lower rated titles usually have fewer. This indicates that users who enjoy a show are more likely to add it to their favorites list. Because favoriting a show requires more intention than simply rating it, this relationship suggests that high ratings reflect deeper engagement.

The correlation coefficient that we calculated reinforces the pattern seen in the visualization. Although the relationship is not perfect, it is strong enough to show that ratings and favorites move together in a meaningful way. The presence of a positive correlation means that the two variables are connected, but not identical. Some highly rated shows do not have extreme favorite counts, while some titles with large fan followings may not have the highest ratings. This reveals the complexity of user behavior. Ratings capture perceived quality, while favorites capture emotional attachment.

Together, these findings show that anime users tend to rate shows positively and that high rated shows often become favorites. This supports our expectations while still leaving room for future exploration. Our findings demonstrate that even simple variables can provide insight into media consumption behavior and that these patterns can be evaluated using reproducible workflows.

## **Future Work**
Our project created a strong foundation for understanding how anime ratings relate to favorites, but there are many ways that future work can build on what we developed. As we worked through each stage of the data lifecycle, we found multiple opportunities to expand the scope of the project, refine our workflow, explore new analytical questions, and improve the automation of our data acquisition process.

One area for future work involves expanding our analysis to include temporal trends. Because anime titles span many years and even decades, it would be interesting to examine how ratings change over time. We could explore whether older shows receive different ratings compared to newer shows or whether certain eras of animation tend to produce more highly rated titles. This approach would require incorporating the release year field from our datasets and grouping titles accordingly. It would also allow us to examine how the anime industryâ€™s evolution influences viewer evaluations.

Another direction would be to explore genre based trends. Anime is divided into many genres such as action, romance, science fiction, slice of life, and comedy. Our JSON dataset contains rich genre and theme information that we did not use extensively in our primary analysis. Future work could involve comparing average ratings across genres, analyzing which genres tend to accumulate the most favorites, or identifying whether certain genres have more variability in ratings. This would broaden our research questions and allow us to connect user preferences to genre characteristics.

Future work could also involve a deeper study of popularity metrics. The favorites field provides one form of engagement, but additional fields such as popularity ranking or number of members could provide more depth. Combining these metrics could help us build a more complete picture of why certain shows stand out. It might also allow us to identify shows that have strong fan followings despite average ratings or shows that are critically acclaimed yet not widely favorited.

Another major direction for future development relates to our workflow automation. While we were able to automate the HuggingFace download successfully, the Kaggle API introduced challenges. Future work could involve identifying more reliable Kaggle file identifiers or adapting the script so that it automatically selects the correct file from Kaggle datasets. We could also explore better ways to authenticate with the Kaggle API so that the workflow runs smoothly without requiring manual intervention. Improving this step would make the acquisition stage fully automated and even more reproducible.

We also want to expand the validation steps within our Snakemake pipeline. Currently, our workflow runs each script in order and produces the expected outputs. Future improvements could include automated tests that check for missing values, verify column names, confirm correct file sizes, or validate schema structures. Adding these checks would make the workflow more robust and prevent silent errors. It would also help future users identify problems earlier in the process.

Metadata generation is another area for future enhancement. While we created documentation and a data dictionary, we could extend this by using structured metadata standards such as Schema.org or DataCite. This would improve the discoverability and reusability of our project in a more formalized repository environment. Using these standards would also introduce us to professional data publication practices.

Beyond analytical and workflow improvements, there is room to expand the educational and collaborative aspects of our project. We learned a great deal from working together, especially about organizing tasks, communicating progress, and aligning our workflow. Future collaborations could include version controlled branching strategies, pull request reviews, and automated documentation tools. These additions would help us simulate real world data science teamwork.

Finally, future work could integrate predictive modeling. Once we clean and integrate the data, we could build models that predict favorites based on rating, genre, studio, or other metadata. This would introduce methods such as regression, clustering, or machine learning techniques. These models could reveal deeper behavioral patterns and help identify what factors most strongly influence user engagement.

Overall, future work can greatly expand the scope of our project by incorporating additional metadata, more complex analytical methods, stronger automation, and richer documentation. Each extension would build off the foundations we created in this project and demonstrate how reproducible workflows can evolve over time.

## **Reporducing**
To reproduce our workflow, a user must follow a structured sequence of steps that rebuilds the environment, downloads the raw data, and runs the Snakemake pipeline. We designed these steps so that anyone can replicate our results by following the instructions and using the same data structure that we used.

The first step is to clone our GitHub repository. Cloning the repository ensures that the user receives the same folder structure, scripts, Snakemake file, metadata, documentation, and placeholders that we created. Once the repository is cloned, the user must navigate into the project directory.

Next, the user must create the correct folder structure. While our repository already contains empty placeholder files, users should confirm that the directories include data/raw, data/processed, results, and db. These folders match the organization used by our workflow and allow Snakemake to recognize where inputs and outputs belong.

After the folders are verified, the user must obtain the raw datasets. Because Kaggle and HuggingFace impose license restrictions, we do not store the raw files in GitHub. Instead, we provide a Box link so that teaching staff can download the exact files we worked with. The user needs to download anime.csv and anime_full_data.json from the Box link and place them inside data/raw. The filenames must match our expected names so that the workflow runs correctly.

The next step is to install the required dependencies. Our repository includes a requirements.txt file that lists all Python packages needed for the workflow. Users can install these dependencies in a clean environment to prevent conflicts. Installing the correct packages is essential for reproducibility, since different versions might cause the scripts to behave differently.

Once the environment is set up and the raw data is in place, the user can run the workflow. Running Snakemake triggers the acquisition, integration, cleaning, and analysis steps in the correct order. The workflow produces new files in data/processed and generates visualizations and correlation results inside the results folder. Snakemake ensures that each step only runs when necessary and that outputs reflect the most recent inputs.

After the workflow completes, users can navigate to the results folder to examine the visualizations and correlation summary. These represent the final answers to our research questions. Users may also explore intermediate files in data/processed to understand how the data transforms throughout the workflow.

By following these steps, anyone can reproduce our analysis from start to finish. This sequence reflects the core principles of data reproducibility and demonstrates how structured workflows support transparency and clarity.

## **References**
1. Kaggle Dataset. Anime Dataset 2025 (Author: Rafid Ahmed). Retrieved from https://www.kaggle.com/datasets/rafidahmed816/anime-dataset-2025
2. HuggingFace Dataset. Anime Titles Dataset. Retrieved from https://huggingface.co/datasets/realoperator42/anime-titles-dataset
3. Snakemake Workflow Management System. Retrieved from https://snakemake.readthedocs.io
4. Python Software Foundation. Python Language Reference. Retrieved from https://www.python.org
5. Pandas Development Team. Pandas Documentation. Retrieved from https://pandas.pydata.org

## **Box**
1. Input Files (raw data): https://uofi.box.com/s/3pe0ahrmk3szcoq81rr4a8xawknadyzu
2. Output Folder (visualizations): https://uofi.box.com/s/6xhwtdmpfmuls4nwk0p22mlpwoe0hubq
